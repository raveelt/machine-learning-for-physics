{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d81ff16-ca1d-460a-a4eb-fccf0b896fbd",
   "metadata": {},
   "source": [
    "# Lab Notebook 22: Discovery of exoplanets\n",
    "\n",
    "We will investigate a problem very common in astrophysics, the detection of exoplanets. Exoplanets are planets outside of our solar system that orbit any star other than our sun. By definition exoplanets do not emit light themselves, which makes their detection quite tricky. A common technique is using the transit method, which investigates the brightness of a star over time. If a star possesses a planet orbiting it, the brightness will reduce periodically, when the planet is transiting our view axis.\n",
    "\n",
    "The data we use here is derived from observations made by the NASA Kepler space telescope.\n",
    "\n",
    "**Training set (exoTrain.csv):**\n",
    "\n",
    "5087 rows or observations  \n",
    "198 columns or features  \n",
    "Column 1 is the label vector, columns 2 - 3198 are the flux values over time\n",
    "\n",
    "37 confirmed exoplanet-stars and 5050 non-exoplanet-stars\n",
    "\n",
    "**Testset (exoTest.csv):**\n",
    "\n",
    "570 rows or observations  \n",
    "3198 columns or features  \n",
    "Column 1 is the label vector, columns 2 - 3198 are the flux values over time\n",
    "\n",
    "5 confirmed exoplanet-stars and 565 non-exoplanet-stars\n",
    "\n",
    "We will now preprocess the data and then train a convolutional neural network to find these exoplanets. This is a hard problems since the exoplanets are so rare. As a result, the dataset is very imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd61040-b970-4d0b-b8c0-0523f409961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential #the model is built adding layers one after the other\n",
    "from keras.layers import Dense #fully connected layers: every output talks to every input\n",
    "from keras.layers import Dropout #for regularization\n",
    "\n",
    "from sklearn import metrics as sk_met\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965efe4c-8ec5-4dae-9177-cc883ab9def0",
   "metadata": {},
   "source": [
    "## Data acquisition\n",
    "\n",
    "First, read in the datasets into pandas frames and check out summary statistics via describe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27773cf8-6859-47da-b968-b5760bf69f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f248b72b-0bc5-4b5f-83fb-9e4deff18f51",
   "metadata": {},
   "source": [
    "Reformat the data into Xtrain, ytrain, Xtest, ytest arrays. Note that the class labels here are 1 and 2, so it is best to subtract 1 so get the more conventional labels 0 and 1.\n",
    "\n",
    "Plot the flux intensity over time for one example of an exoplant star (eg. index 0) and a non-exoplanet star (eg. index 100). You should see a qualitative difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e685b9-6a39-4639-b00d-b080306acd30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6da560ee-5c7b-4406-ad15-5f664cde71a6",
   "metadata": {},
   "source": [
    "## Data preparation for CNN via FFT, scaling and filtering\n",
    "\n",
    "In one of the last exercises, we used a convolutional neural network (CNN) to classify the MNIST dataset. The superior performance of this approach comes from the fact that the filters applied to the input gather and aggregate information of regions rather than just looking at individual values or pixels. This can also be of use in our problem, however we have to make some transformations to make the data more suited for the classification task.\n",
    "\n",
    "The signature we are looking for is a periodic reduction of the light intensity, signaling the transit of an exoplanet. It would be beneficial if we could use this knowledge to extract the relevant information from the raw data. Looking at the Fourier transformed version of the data therefore is a straightforward first step.\n",
    "\n",
    "1. Apply a **fast fourier transform** (from scipy.fftpack) to Xtrain and Xtest. The result is complex so take the absolute value. Since the fft is symmetric around zero, it is enough to keep the first half of the spectrum only.\n",
    "2. Use *normalize* from sklearn.preprocessing to make each data observation a vector of length 1. This reduces large values\n",
    "3. Use *gaussian_filer* (from scipy.ndimage) with gamma=10 to reduce the noise\n",
    "4. Lastly, use MinMaxScaler from sklearn.preprocessing to keep the data between 0 and 1.\n",
    "\n",
    "   Show a plot of the data for the same two stars you looked at above after all this processing and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de367f5-528c-4af2-b375-232b5af128fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b34234-cd21-4506-aa08-c3c24545181b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1290dbd3-41c0-4805-ba87-109c7b09ed8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5fb07ed-a317-4101-a96a-04f7e903f772",
   "metadata": {},
   "source": [
    "## Train a CNN\n",
    "\n",
    "Use train_test_split to split off 20% of the training data as a validation set (the test data stays unchanged).\n",
    "\n",
    "In order to use a CNN, we first have to transform our training data, which means reshaping the training set to have the dimensions (4069, 1599, 1), and similar for the test set.\n",
    "\n",
    "Since the data set is so imbalanced, we will try to use the class_weight parameter to give more weight to the rare class. First, compute the \"balanced\" class weights. You can use the utility at https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html or write your own. See also https://www.tensorflow.org/tutorials/structured_data/imbalanced_data for how the class_weight dictionary needs to look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a426072-b865-4fe8-a2a6-0d02b9a26a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7f14a-c384-4da2-ad19-73580c2dd412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8966e30a-70b3-45ae-adde-8e1a5f322657",
   "metadata": {},
   "source": [
    "**Proposed structure of the CNN:**\n",
    "\n",
    "1. A 1d convolution layer (Conv1D) with 16 filters, input shape matching the dataset, kernel_size=3, activation = 'relu', kernel_regularizer='l2', padding='same'.\n",
    "2. A max pooling layer (MaxPooling1D) with pool_size=2 and strides =2\n",
    "3. A 30% dropout layer\n",
    "4. A flatten operation\n",
    "5. A dense layer with 32 neurons\n",
    "6. a 50% dropout layer\n",
    "7. A final output layer with output dimension 2 and sigmoid activation for classification. The appropriate loss function is then *keras.losses.SparseCategoricalCrossentropy()*\n",
    "\n",
    "Use the adam optimizer with learning rate = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e3f0c-d4c0-4852-bd8d-a16dec99f569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "771bc80e-d05c-4f38-afbc-0987beb86f1c",
   "metadata": {},
   "source": [
    "**Try training the model for 20 epochs** Remember to pass the \"validation_data\" and the \"class_weight\" to model.fit(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c0f3fd-1756-4fee-b329-a0a2c5e45702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8764bfb-2397-4b78-aa68-8d09785cdfeb",
   "metadata": {},
   "source": [
    "Plot the training and validation loss and accuracy versus epochs. Have you trained enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5dce65-c9c7-405c-a675-7c4e5356686d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a425282-e0ed-4237-b09f-f6160fad43b7",
   "metadata": {},
   "source": [
    "Inspect the confusion matrix for the CNN on the test data. Note that in keras, calling model.predict gives you the output probabilities, not the class label. You need to compute that from the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7c4bf-8e3c-423b-9b35-546beb730c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "818a95fa-5a03-4649-9200-e493d2354401",
   "metadata": {},
   "source": [
    "Compute accuracy, precision, and recall metrics for train and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd999a44-24f1-49a4-b6ef-964b26b307bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ce217cd-94cc-4993-8de0-782a10a76011",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Which metric, in your opinion, is most useful for the present problem? Do you have a useful classifier?\n",
    "2. Re-train the network a few times, and comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547e621-145a-40fa-85df-42dd31f5658f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ee6fd06-b458-40b5-b6cd-6053586078b1",
   "metadata": {},
   "source": [
    "## Train a RNN\n",
    "\n",
    "Our time series data is in principle amenable to treatment by an RNN. To this end, we **return to the original, unprocessed data** without the FFT and filtering. \n",
    "\n",
    "Use again train_test_split to split off 20% of the training data as a validation set (the test data stays unchanged). Then reshape the features so that X_train for instance has the shape (4069, 3197, 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb11ee6-e8ed-42ea-bfdf-e7858b3dab30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79a3f884-42dd-4520-aa32-0bcf9519b05d",
   "metadata": {},
   "source": [
    "Let's build a sequential network with \n",
    "\n",
    "1) a SimpleRNN layer of 16 of 32 neurons.\n",
    "2) a dropout layer with 20%\n",
    "3) a final dense layer with sigmoid activation as before for classification\n",
    "\n",
    "Everything else can be the same as the CNN, including the class weights.\n",
    "\n",
    "**Try training the RNN for 10 epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1de2dd-c743-43d7-9b06-e7056655bed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "305211b8-a7d3-4649-84d4-9610f9df58e4",
   "metadata": {},
   "source": [
    "Plot again training and validation loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab1463b-3dda-4a77-9054-7a45d25d9608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d36689a8-e35f-454a-9890-ebac30a3c3ac",
   "metadata": {},
   "source": [
    "And the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825fd817-a59d-4389-be2c-53c5fd74dcf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0556cd1d-66e5-44ee-b597-4b2f114d9750",
   "metadata": {},
   "source": [
    "And the final scores. How is the RNN doing in our classification task? Is the RNN or CNN better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9604b5-9e99-4315-b971-6d6234dfa1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d980d1-41f3-4ed3-893a-4cc3324228fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
