{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIb7AW-J_Q0j"
   },
   "source": [
    "# Lab Notebook 19\n",
    "\n",
    "Today, we develop a fully connected neural network to solve a previously seen problem, the particle ID classification.\n",
    "\n",
    "Based on: *Copyright: Viviana Acquaviva (2023). License: [BSD-3-clause](https://opensource.org/license/bsd-3-clause/)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCi2a2GB_Q0m"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: 4top vs ttbar with neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAX7Ddfd_Q0n"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential #the model is built adding layers one after the other\n",
    "from keras.layers import Dense #fully connected layers: every output talks to every input\n",
    "from keras.layers import Dropout #for regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AddTCBp6_Q0o"
   },
   "source": [
    "We begin with the 4top vs ttbar problem, and we use the configuration where we added the features \"number of leptons\", \"number of jets\" etc. For reference, the optimal SVM achieved 94-95% accuracy. Note that those numbers had not been run through <b> nested </b> cross validation so they might be slightly optimistic. \n",
    "\n",
    "Read 'Features_lim_2.csv' in to a dataframe, and read 'Labels_lim_2.txt' into an array. You may find it helpful to look at the shape, elements, etc. of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdeqbK8B_Q0o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eo9nE9rL_Q0r"
   },
   "source": [
    "## Step 1.2\n",
    "\n",
    "There is no \"built-in\" cross validation process here, so we would need to build it ourselves. For now, we can build three sets: train, validation (for parameter optimization), and test (for final evaluation). We should ideally build this as a cross-validation structure.\n",
    "\n",
    "Use \"shuffle\" from sklearn to shuffle X and y, specifying a random state of 10. Let X_train be the first 3000 values, let X_val be the next 1000 values, and let X_test be the remaining values. Create y_train, y_val, and y_test in like manner. Print the shape of all arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGV2FTWj_Q0y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6644VLi_Q0z"
   },
   "source": [
    "## Step 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ws9Ko4wv_Q0z"
   },
   "source": [
    "Let's think about the model architecture.\n",
    "\n",
    "1. Our input layer has **24 neurons**. \n",
    "\n",
    "2. Our output layer has **1 neuron** (the output is the probability that the object belongs to the positive class). We could also set it up as 2 neurons (and have softmax as the final non-linearity), but this is redundant in a binary classification problem.\n",
    "\n",
    "3. We will add one hidden layer. Here we are making **size = 20** (we should optimize this hyperparameter!). We can also reserve the possibility of adding a dropout layer after each one. The dropout fraction should also be optimized through CV.\n",
    "\n",
    "Other decisions that we have to make are: which nonlinearities we use (for now: ReLU for hidden layers, sigmoid for the final one), which optimizer we use (Adam), which starting learning rate we adopt (here 0.001, but again this should be decided through CV), the number of epochs (e.g. 100; we can plot quantities of interest to check that we have enough), the batch size for the gradient descent step (here 200, but can explore!) and the loss function. The latter is the binary cross entropy, which is the standard choice for classification problems where we output a probability. It rewards \"confidence\" in a correct prediction (high probability). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1685280325392,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "zUYax3cN_Q00",
    "outputId": "edf3b261-77f1-423d-8414-2f34e8487132",
    "scrolled": true
   },
   "source": [
    "Use the \"dir\" command to print the possible choices of optimizers in keras. Also, print the possible losses in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1685280325392,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "zUYax3cN_Q00",
    "outputId": "edf3b261-77f1-423d-8414-2f34e8487132",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppQRvorFEKRc"
   },
   "source": [
    "A standard choice for a case like ours, where the labels are 0/1 but we can predict a probability, is the binary cross-entropy or log loss:\n",
    "\n",
    "L = - $\\frac{1}{N} \\sum_{i=1}^N y_i \\cdot log(p(y_i)) + (1-y_i) \\cdot log (1 - p(y_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpxfJMetEn6K"
   },
   "source": [
    "p is the probability that an object belongs to the positive class. It penalizes positive examples that are associated with predicted low probability, and negative examples that are associated with predicted high probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yB9fFu4iTjl"
   },
   "source": [
    "Finally, print the list of the available activation functions from keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 102,
     "status": "ok",
     "timestamp": 1685280332901,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "0wbZTVBd_Q00",
    "outputId": "15abe46b-83f5-4d2a-e343-71800748257e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otwhTPnm_Q01"
   },
   "source": [
    "## Step 1.4\n",
    "\n",
    "Given the architecture above, build this sequential neural net with keras. You can model the setup on last week's lab. Note that here you don't need to flatten the input (it is already flat), so the input layer should just be another dense layer with the correct input shape. You can either define the whole network (as in lab 18) or define **model = Sequential()** and then use the **model.add()** method to add layers.\n",
    "\n",
    "*Note: The \"metric\" keyword here serves to specify other possible metrics we would like to monitor. The loss itself is not interpretable, so we'll keep an eye on the accuracy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woqxQqX1_Q01"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lkvPwRy_Q01"
   },
   "source": [
    "Next, we'll move on to the fitting procedure. Define your neural network as the output of \"model.fit\" on your training data. Add the validation data as well, and set epochs=100 and batch_size=200.\n",
    "\n",
    "*Note:* \n",
    "1. *\"epochs\" is the number of back-and-forth passages through the network.*\n",
    "2. *\"batch size\" is how many of the data are used at every step in updating weights.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22205,
     "status": "ok",
     "timestamp": 1685280461177,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "4a2GD11m_Q01",
    "outputId": "e74ce4e4-9722-4f5d-ad46-456ff7c6d4d6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gSnZtIbBXgQ"
   },
   "source": [
    "How does the training and validation accuracy look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmdX7xQI_Q01"
   },
   "source": [
    "## Step 1.5\n",
    "\n",
    "It's helpful to plot how training and validation loss vary throughout the epochs.\n",
    "\n",
    "First, plot the loss as a function of epoch for the training and validation data sets. Next, plot the accuracy as a function of epoch for the training and validation data sets. We already did this for the MNIST dataset example in the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "executionInfo": {
     "elapsed": 1445,
     "status": "ok",
     "timestamp": 1685280526737,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "ekaBUX5j_Q01",
    "outputId": "489da061-4d85-4185-f133-d46fb0b9457d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJLdMkyp_Q01"
   },
   "source": [
    "We can see how plotting the accuracy, which is interpetable, is more useful than plotting the loss. The wild oscillations observed in the validation loss, as well as the absolute value of the loss after 100 training epochs, are not great signs. \n",
    "\n",
    "How can we fix this?\n",
    "\n",
    "Let's look at the data again using X.describe : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1685280664606,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "PzmnBG7__Q01",
    "outputId": "25f4500e-f3b9-4378-b275-d4e3a33802fe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRv36Ye0tXMo"
   },
   "source": [
    "## Step 1.6\n",
    "\n",
    "We forgot scaling (and our features have wildly different ranges)! Use StandardScaler as usual. Remember, we only use the training set to derive the scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qQMC772_Q02"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define new training, validation, and test arrays for X that have been scaled using \"transform\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nczuqWMB_Q03"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lg0XCSX8kYaH"
   },
   "source": [
    "We can now train our neural network again. As before, use \"model.fit\", but this time with the new X arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10651,
     "status": "ok",
     "timestamp": 1685280907189,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "ZIBbWjR9_Q03",
    "outputId": "cf4438d5-4d1a-4cb0-d551-815fbefdf88f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracies should look much better now! Make the two plots from Step 5 below, now with your new results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2637,
     "status": "ok",
     "timestamp": 1685280917181,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "fjSveqjQ_Q03",
    "outputId": "68fff88e-7f67-4e0e-d66b-8dc4177adf4b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWx9wWQgkvIC"
   },
   "source": [
    "As you can see, this network is much better behaved, and it achieves a final accuracy similar to the one found by SVMs (this is common for tabular data like ours). We do see some signs of high variance in the accuracy/validation curve; some regularization technique, may help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.7\n",
    "\n",
    "The final evaluation of the model is always done on the test set; the reason is that the validation fold is used for hyperparameter optimization (which we haven't done here), and test set is blind to it.\n",
    "\n",
    "Define \"scores\" as the output of \"model.evaluate\", and specify \"verbose=1\". Print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Regularization\n",
    "\n",
    "There are several options for regularization; here we explore a few. \n",
    "\n",
    "First, add a **dropout layer** via the command\n",
    "\n",
    "model.add(Dropout(0.2)) #This is the dropout fraction\n",
    "\n",
    "after both first and hidden layer. Then train this model and store the training history for later plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zF4Dll3N_Q04"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11375,
     "status": "ok",
     "timestamp": 1685281165975,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "g5WAqdzI_Q04",
    "outputId": "ffcfc384-0927-43a5-87d1-0c5a4b271c6e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets remove the dropout layers and instead add an **L2-norm penalty** to the weights, similar to what is done in linear regression. You can do this via the\n",
    "\n",
    "kernel_regularizer=keras.regularizers.l2(0.001)\n",
    "\n",
    "parameter in model.add(). Train this model and save the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate comparison plots of losses and accuracy for the three models (regular, dropout, L2) combined for ease of comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "executionInfo": {
     "elapsed": 1820,
     "status": "ok",
     "timestamp": 1685281179753,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "__hX7-bv_Q04",
    "outputId": "c19d72a9-9c44-4b3c-d9dd-6422c6251163"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 156,
     "status": "ok",
     "timestamp": 1685281414037,
     "user": {
      "displayName": "Viviana Acquaviva",
      "userId": "16294609486294432741"
     },
     "user_tz": 240
    },
    "id": "PtWa7nxw_Q04",
    "outputId": "a65b9234-8e03-4726-ab20-d39a1780267f"
   },
   "source": [
    "# Part 3: Study of Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the NN is can also be strongly affected by the choice of optimizers. Let's train five different models with five different optimizers:\n",
    "\n",
    "1) SGD (stochastic gradient descent, default parameters)\n",
    "2) Momentum (= SGD with momentum=0.5)\n",
    "3) RMSprop\n",
    "4) Adagrad\n",
    "5) Adam\n",
    "\n",
    "Note that for SGD the default learning rate is set to 0.01, while it is 0.001 for the other ones.\n",
    "\n",
    "Train the five different models and store the training history. After that, make a plot of the training accuracy vs history for these five optimizers. Then make the same plot again for the validation accuracy. Which optimizer is best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
